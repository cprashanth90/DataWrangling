{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeRateDogs Twitter Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to wrangle and analyze the tweets and retweet counts of dog ratings data. WeRateDogs is a great archive, but appears to contain very basic information with regard to the tweet counts, etc..\n",
    "\n",
    "The missing tweet stats can be completed by making use of the Twitter API where we can query for a given tweet ID and Twitter API will return us a JSON data with other information regarding the tweet.\n",
    "\n",
    "We are also provided with image predictions of what breed a particular dog belongs to, in a separate file on image_predictions.tsv. These predictions are run using a Neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data that we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are already provided with the following datasets:\n",
    "1. Tweet data from the WerateDogs Twitter Archive - in a .csv file (twitter-archive-enhanced.csv).\n",
    "2. Predictions from a neural network model that details predictions of dog breed and the score of the predictions. This is from a tsv file saved at \"https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WeRateDogs Twitter Archive contains observations of 2375 tweets. There are 17 columns in this original dataset. There were several quality issues in this twitter archive and a few tidiness issues found. \n",
    "\n",
    "The Neural network predictions dataset is a table of 2075 observations with 12 columns. Each observation is a tweet image - contains the top 3 dog breed prediction, confidence levels and an indicator if the predicted image is a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WeRate Dogs Twitter Archive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below I provide an assessment of the quality issues found in the WeRateDogs Twitter Archive dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Inappropriate types for Timestamp columns**\n",
    "  - Assess: \"timestamp\" column in werate_dogs_twitter_data is of type str. Converting this to object timestamp can be useful for us   while analyzing the data.\n",
    "  - Clean: Convert \"timestamp\" column to datetime object using pandas to_datetime method.\n",
    "\n",
    "**2. Inappropriate datatypes**:\n",
    "  - Assess: Columns retweeted_status_id, retweeted_user_id are floating point numbers type (np.float64) in scientific notation. This poses a serious issue when querying the Twitter API for data. It will be very helpful to convert these to integers.\n",
    "  - Clean: Convert datatypes of retweeted_status_id, retweeted_user_id from np.float64 to np.int64.\n",
    "\n",
    "**3. Missing Values in certain important columns**\n",
    "- Missing values in retweeted_status_id, retweeted_user_id, retweeted_status_timestamp. Ideally, we cannot do anything about this because the twitter API cannot help us with providing information on these values. But, if we were to use a NULL value to retrieve data from Twitter's API, we get errors. We need to mitigate this.\n",
    "- Clean: For better exception management, we can set these NULL values to a value of -1. With a TweetID value = -1, Tweepy returns a constant Exception with code = 8. Set all types of NULL values in columns 'retweeted_status_id', 'retweeted_user_id' to a value -1.\n",
    "\n",
    "**4. Missing/non-existing tweet data**\n",
    "- Some \"tweet_id\" values result in missing/non-existing tweets and are in some cases deleted tweet IDs. For example: in index #21, Tweet ID: 888202515573088257 doesnt exist. We can however attempt to get the details of this tweet from the expanded_url column.\n",
    "- Clean: Retreive correct tweet ID's from the twitter API using the part of the tweet from the exapnded_url columns.\n",
    "\n",
    "**5. Incorrect dog names**\n",
    "- dog names Null values: 'name' column contains values such as \"None\". It appears that the parsing of the names haven't been successful. We can treat these as missing values.- \n",
    "- dog names unclean values: There are values in the 'name' column such as \"a\", \"getting\", \"actually\", etc.. These are not real dog names. These appear to be errors from trying to parse out the names of dogs.\n",
    "- Clean: Fill in dog names for such unclean values using the corrected dog names in a csvfile. The CSV file contains three columns, Index number of the corresponding tweet, value to be searched for and value to be replaced with.\n",
    "\n",
    "**6. Bad Rating Numerators and Denominators**:\n",
    " - There are some unclean data with the Rating Numerators and denominators. There is one tweet with a rating of 9/10 but the numerator and denominator columns are 24,7 respectively and 1 tweet with a denominator value equals to 0. These appear to be parsing errors.\n",
    " - Also, there are some rows in the data set where decimal ratings of the numerators are wrongly parsed. For example, tweet ID: 883482846933004288 actually has a ranking of 13.5, but it is wrongly parsed as 5.\n",
    " - Clean:  <br>\n",
    " - Spot check manually and clean up such data points with pandas replace.\n",
    " - Use regular expressions to identify rows with decimal ratings.\n",
    "\n",
    "**7. No standardization of Ratings**:\n",
    " - There are some values of rating denominators in the dataset that are as high as 170. The corresponding rating numerator for this dog is 204. This clearly indicates that the dog is rated at 12/10 or a 120 on a percentage score basis. Standardinzing will help us with analysis.\n",
    " - Clean: If rating denominators are greater than 10, then Standardize Rating numerator as follows: (rating_numerator)*10/(rating_denominator)\n",
    " \n",
    "**8. Mistakes or outlier values in the rating_numerators columns:**\n",
    "  - Some dogs are rated very highly or probably there is a clear bias or is done as a joke, such as the one that is rated at 420/10 or 1776/10. \n",
    "  - Clean: Drop these rows that have such a rating threshold of standardized numerator rating more than 20.\n",
    "  \n",
    "**9. We aren't really using the Retweet and replies data:**\n",
    "  - Retweet statuses are not going to be a part of our analysis. Also there are too many null values in these columns. Only 181 are not null. We can drop these columns once we are done retrieving data from the Twitter API. The same applies to in_reply_to_user_id and in_reply_to_status_id columns in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a summary of the assessment and cleaning of the Tidiness issues in the WeRateDogs Twitter Archive dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Column 'dog stage' missing: 4 variable values as column headers**\n",
    "-  Columns doggo, floofer, pupper, puppo are values and not variable names; this violates the 3rd rule of Tidy data. Each observational unit does not form a table. There are 14 observations in the dataset for which there are multiple dog stages. \n",
    "- Clean: Melt columns \"dogg\", \"floofer\", \"pupper\" ,\"puppo\" into 1 dog_stage column. For multiple value dog-stage value, insert a \"-\" between the stages.\n",
    "\n",
    "**2. Column \"source\" contains 2 variables in a single column (source_url, source_description)**:\n",
    "- Column \"source\" in the WeRateDogsTwitter Archive data contains two variables, the website reference of the source and the description of the source as a HTML tag.\n",
    "- Column 'expanded_urls' contains multiple URLs, some of which are repetitions of the same photo URLs. Remove this column from the twitter archive dataset and create a new dataframe for the mediaURLs.\n",
    "\n",
    "**3. Column rating_Denominator is redundant after standardization**\n",
    " - Drop column rating_denominator after standardiing the rating_numerator column. \n",
    " \n",
    "**4. Column 'expanded_urls' contains multiple URLs, some of which are repetitions of the same photo URLs.** \n",
    "  - Remove this column from the twitter archive dataset and create a new dataframe for the mediaURLs. These can be merged later when out dataset obeys the tidy data conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the Predictions dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an assessment of the data stored in the **predictions** dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quality**\n",
    "\n",
    "- Column \"tweet_id\" is of type object. Each tweet_id is a string, when it will be useful to have tweet_id as an integer.\n",
    "\n",
    "- Columns with predictions, i.e \"p1_dog\",\"p2_dog\", \"p3_dog\" are strings. it will be useful to have these objects as a bool (True/False) data type.\n",
    "\n",
    "- Columns with predicted confidence values p1_conf, p2_conf, p3_conf are also of type object. It will be useful to convert these into float64 data type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tidiness**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One issue with the Predictions dataframe is that variable values appear in column names. Columns pn, pn_conf, pn_dog where n=1,2,3 can be melt into a table structure as follows: 'predictionlevelrank', 'PredictedValue', 'confidence value', \"is dog\" columns in the 'predictions' dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Predictions Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Data from the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we retrieve twitter data from the Twitter API for each tweet ID listed in the weratedogs twitter Archive. There appears to be some tweet ID's from our original twitter archive csv file, for which the tweets have been deleted. Such tweets do not carry other info, such as # of Retweets, likes etc.. I have collected such tweets in file \"unauthorized_tweet_ids.csv\". For example, Tweet ID 680055455951884288 appears to be deleted or not existing anymore.\n",
    "\n",
    "There also appear to be certain Tweet ID's that do not contain a valid ID, however, the retweet ID/the tweet ID found in the media URL is valid and turns out that these tweets are not really retweets and are tweeted by User @dog_rates.\n",
    "For example, tweet ID: 770743923962707968 doesnt exist. URL: https://twitter.com/dog_rates/status/770743923962707968 doesnt exist. However, the media URLs associated with this row in archive returns a valid tweet, which is at the website:\n",
    "https://twitter.com/dog_rates/status/739238157791694849/video/1 , where we see this dog blowing bubbles :) \n",
    "The below code collects the JSON object pertaining to the tweet ID found from the Media URL/ retweeted ID.\n",
    "\n",
    "Overall there are 5 tweet ID's in the archive that are unauthorized/deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON tweet data from each tweet was output to a file Tweet-json.txt. Each line of thist txt file is a string that is of a JSON dictionary notation containing information such as # retweets, favorite counts, etc..\n",
    "Now using this txt file, I create a dataframe whose columns correspond to the tweet ID of a tweet, Retweet Count, Favorite Count, mediaURLs associated, retweeted (yes/no). This dataframe was merged with our original twitter archive master dataframe and the resulting dataframe was written to a .csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``master_data.csv``: This contains the output of the master dataframe obtained by merging the original twitter archive dataframe and the dataframe obtained from retrieving data from the Twitter API.<br>\n",
    "``predictions_cleaned.csv``: The cleaned Predictions data obtained from the .tsv file. <br>\n",
    "``mediaURLs.csv``: This is the expanded_urls column, that violated the Tidy Data principle. This file contains the mapping of a tweet ID to the URLs that it contains. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
